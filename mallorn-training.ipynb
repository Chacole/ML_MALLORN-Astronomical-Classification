{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14223785,"sourceType":"datasetVersion","datasetId":9073653}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mallorn Challenge: Astronomical Time-Series Classification\n## Model: Feature Engineering + LightGBM\n**Author:** AI Assistant\n**Dataset Structure:** `split_xx` folders containing Lightcurves (Time, Flux, Filter).\n\n### T·ªïng quan\nNotebook n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ri√™ng ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu Lightcurves (CSV):\n1.  **Data Loading:** T·ª± ƒë·ªông qu√©t to√†n b·ªô c√°c folder `split_01`...`split_20` ƒë·ªÉ gom d·ªØ li·ªáu.\n2.  **Feature Engineering:** Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng th·ªëng k√™ (Min, Max, Mean, Std, Skew) cho t·ª´ng d·∫£i s√≥ng (u, g, r, i, z, y).\n3.  **Modeling:** S·ª≠ d·ª•ng **LightGBM Classifier** - thu·∫≠t to√°n SOTA cho d·ªØ li·ªáu b·∫£ng/chu·ªói th·ªùi gian d·∫°ng n√†y.\n4.  **Handling Imbalance:** T·ª± ƒë·ªông x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu (Class 0 vs Class 1).","metadata":{}},{"cell_type":"markdown","source":"## ƒê·ªÉ Resume gi·ªØa c√°c session kh√°c nhau (Session A b·ªã t·∫Øt -> M·ªü Session B ch·∫°y ti·∫øp):\n\n### Ch·∫°y Session 1: Ch·∫°y b√¨nh th∆∞·ªùng. Code tr√™n s·∫Ω t·∫°o ra c√°c file extracted_features.csv v√† lgb_model_fold_0.txt, lgb_model_fold_1.txt... trong ph·∫ßn Output.\n\n### L∆∞u Session 1: K·ªÉ c·∫£ khi ch∆∞a ch·∫°y xong, n·∫øu b·∫°n ·∫•n Save Version (ho·∫∑c n·∫øu session time out nh∆∞ng c√≥ file output), Kaggle s·∫Ω l∆∞u l·∫°i c√°c file ƒë√≥.\n\n### T·∫°o Dataset t·ª´ Output:\n- V√†o tab Output c·ªßa Notebook Session 1.\n- ·∫§n \"New Dataset\" (ho·∫∑c \"Create Dataset\"). ƒê·∫∑t t√™n v√≠ d·ª•: mallorn-training-part1.\n\n### M·ªü Session 2 (Resume):\n- M·ªü l·∫°i notebook.\n- ·ªû c·ªôt b√™n ph·∫£i, ·∫•n Add Input -> Ch·ªçn Your Datasets -> Ch·ªçn mallorn-training-part1.\n- S·ª≠a Cell 3:\nCFG.RESUME_PATH = '/kaggle/input/mallorn-training-part1' \n- ·∫§n Run All.\n\nK·∫øt qu·∫£: Code s·∫Ω t·ª± ƒë·ªông b·ªè qua b∆∞·ªõc extract_features (load t·ª´ file CSV) v√† t·ª± ƒë·ªông b·ªè qua c√°c Fold ƒë√£ c√≥ file .txt. N√≥ s·∫Ω ch·ªâ train ti·∫øp c√°c Fold c√≤n thi·∫øu.","metadata":{}},{"cell_type":"markdown","source":"1. Imports & Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# C·∫•u h√¨nh hi·ªÉn th·ªã\npd.set_option('display.max_columns', 500)\nplt.style.use('seaborn-v0_8-darkgrid')\n\nprint(f\"LightGBM version: {lgb.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:03:49.715845Z","iopub.execute_input":"2025-12-19T14:03:49.716141Z","iopub.status.idle":"2025-12-19T14:03:49.722865Z","shell.execute_reply.started":"2025-12-19T14:03:49.716119Z","shell.execute_reply":"2025-12-19T14:03:49.721844Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. CONFIG & Helper Functions","metadata":{}},{"cell_type":"code","source":"class CFG:\n    # ƒê∆∞·ªùng d·∫´n (T·ª± ƒë·ªông t√¨m)\n    INPUT_ROOT = '/kaggle/input'\n    WORKING_DIR = '/kaggle/working'\n\n    # N·∫øu ch·∫°y l·∫ßn ƒë·∫ßu: ƒë·ªÉ None\n    # N·∫øu ch·∫°y l·∫ßn 2: ƒëi·ªÅn ƒë∆∞·ªùng d·∫´n dataset ch·ª©a output l·∫ßn 1 (VD: '/kaggle/input/mallorn-part1-output')\n    RESUME_PATH = None \n    # RESUME_PATH = '/kaggle/input/my-previous-output-dataset'\n\n    # Model Params\n    n_folds = 5\n    seed = 42\n    target_col = 'target'\n    id_col = 'object_id'\n    \n    # LightGBM Hyperparameters\n    lgb_params = {\n        'objective': 'binary', # Ho·∫∑c 'multiclass' n·∫øu > 2 l·ªõp\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt',\n\n        # H·ªçc ch·∫≠m l·∫°i ƒë·ªÉ t√¨m ƒëi·ªÉm t·ªëi ∆∞u t·ªët h∆°n\n        'learning_rate': 0.03, \n\n        # Ki·ªÉm so√°t ƒë·ªô ph·ª©c t·∫°p (Tr√°nh Overfitting)\n        'num_leaves': 31,        # Model ƒë∆°n gi·∫£n h∆°n s·∫Ω t·ªïng qu√°t t·ªët h∆°n\n        'max_depth': -1,          # Gi·ªõi h·∫°n ƒë·ªô s√¢u c√¢y (ƒëang ƒë·ªÉ t·ª± do)\n        'min_data_in_leaf': 20,\n\n        # Regularization (Ph·∫°t model n·∫øu tr·ªçng s·ªë qu√° l·ªõn)\n        'lambda_l1': 0.05,        # L1 Regularization\n        'lambda_l2': 0.05,        # L2 Regularization\n\n        # Sampling (Gi√∫p model kh√¥ng nh√¨n th·∫•y to√†n b·ªô data m·ªói l·∫ßn -> ch·ªëng h·ªçc v·∫πt)\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 1,\n        'n_jobs': -1,\n        'verbose': -1,\n        'seed': 42,\n        'is_unbalance': True # Quan tr·ªçng v√¨ dataset m·∫•t c√¢n b·∫±ng (0 >> 1)\n    }\n\ndef seed_everything(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed_everything(CFG.seed)\n\n# H√†m t√¨m dataset\ndef find_files():\n    \"\"\"Qu√©t to√†n b·ªô th∆∞ m·ª•c ƒë·ªÉ t√¨m file logs v√† lightcurves\"\"\"\n    train_log_path = None\n    test_log_path = None\n    sample_sub_path = None\n    \n    # T√¨m file logs ch√≠nh\n    for root, dirs, files in os.walk(CFG.INPUT_ROOT):\n        if 'train_log.csv' in files:\n            train_log_path = os.path.join(root, 'train_log.csv')\n        if 'test_log.csv' in files:\n            test_log_path = os.path.join(root, 'test_log.csv')\n        if 'sample_submission.csv' in files:\n            sample_sub_path = os.path.join(root, 'sample_submission.csv')\n            \n    print(f\"Train Log: {train_log_path}\")\n    print(f\"Test Log: {test_log_path}\")\n    \n    # T√¨m c√°c file lightcurves trong c√°c th∆∞ m·ª•c split\n    # Logic: C√≥ th·ªÉ c√≥ nhi·ªÅu file tr√πng t√™n trong c√°c split, ta s·∫Ω gom h·∫øt\n    lc_files = glob.glob(os.path.join(CFG.INPUT_ROOT, '**', '*_full_lightcurves.csv'), recursive=True)\n    print(f\"Found {len(lc_files)} lightcurve files.\")\n    \n    return train_log_path, test_log_path, sample_sub_path, lc_files\n\ntrain_log_path, test_log_path, sample_sub_path, lc_files = find_files()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:03:49.723945Z","iopub.execute_input":"2025-12-19T14:03:49.724707Z","iopub.status.idle":"2025-12-19T14:03:49.847660Z","shell.execute_reply.started":"2025-12-19T14:03:49.724689Z","shell.execute_reply":"2025-12-19T14:03:49.847025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Data Loading & Merging","metadata":{}},{"cell_type":"code","source":"print(\"=== Loading Metadata (Logs) ===\")\n# ƒê·ªçc Metadata (Labels, Z, EBV...)\nif train_log_path:\n    train_meta = pd.read_csv(train_log_path)\n    print(f\"Train Meta Shape: {train_meta.shape}\")\n    display(train_meta.head(3))\nelse:\n    raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y train_log.csv!\")\n\nif test_log_path:\n    test_meta = pd.read_csv(test_log_path)\n    print(f\"Test Meta Shape: {test_meta.shape}\")\nelse:\n    print(\"Warning: Test log not found.\")\n\nprint(\"\\n=== Loading Lightcurves (This may take a while) ===\")\n# Chi·∫øn thu·∫≠t: ƒê·ªçc sample ƒë·ªÉ ki·ªÉm tra c·ªôt, sau ƒë√≥ ƒë·ªçc v√† concat\n# L∆∞u √Ω: V√¨ file c√≥ th·ªÉ r·∫•t l·ªõn, ta ch·ªâ ƒë·ªçc nh·ªØng c·ªôt c·∫ßn thi·∫øt\ndfs = []\ncols_to_use = ['object_id', 'Time (MJD)', 'Flux', 'Flux_err', 'Filter']\n\n# Thanh ti·∫øn tr√¨nh ƒë·ªçc file\nfor f in tqdm(lc_files, desc=\"Reading LC files\"):\n    try:\n        # Ki·ªÉm tra xem file l√† train hay test ƒë·ªÉ t·ªëi ∆∞u (t√πy ch·ªçn)\n        df_chunk = pd.read_csv(f, usecols=lambda c: c in cols_to_use)\n        dfs.append(df_chunk)\n    except Exception as e:\n        print(f\"Error reading {f}: {e}\")\n\nif dfs:\n    full_lc = pd.concat(dfs, ignore_index=True)\n    # Lo·∫°i b·ªè duplicate n·∫øu c√°c split ch·ª©a d·ªØ li·ªáu tr√πng l·∫∑p\n    full_lc = full_lc.drop_duplicates()\n    print(f\"Total Lightcurve Points: {len(full_lc)}\")\n    display(full_lc.head())\nelse:\n    raise ValueError(\"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu Lightcurve n√†o!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:03:49.848282Z","iopub.execute_input":"2025-12-19T14:03:49.848522Z","iopub.status.idle":"2025-12-19T14:03:52.148232Z","shell.execute_reply.started":"2025-12-19T14:03:49.848496Z","shell.execute_reply":"2025-12-19T14:03:52.147411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. Feature Engineering","metadata":{}},{"cell_type":"code","source":"from scipy.stats import linregress\n\nfeature_file_name = 'extracted_features.csv'\nloaded_features = False\n\n# 1. Ki·ªÉm tra Resume (Session c≈©)\nif CFG.RESUME_PATH and os.path.exists(os.path.join(CFG.RESUME_PATH, feature_file_name)):\n    print(f\"üîÑ Found saved features in Input: {CFG.RESUME_PATH}\")\n    lc_features = pd.read_csv(os.path.join(CFG.RESUME_PATH, feature_file_name))\n    loaded_features = True\n\n# 2. Ki·ªÉm tra Working Dir (Session hi·ªán t·∫°i)\nelif os.path.exists(os.path.join(CFG.WORKING_DIR, feature_file_name)):\n    print(f\"üîÑ Found saved features in Working Directory\")\n    lc_features = pd.read_csv(os.path.join(CFG.WORKING_DIR, feature_file_name))\n    loaded_features = True\n\n# 3. T√≠nh to√°n m·ªõi\nif not loaded_features:\n    print(\"‚ö° No cached features found. Starting extraction (This takes time)...\")\n    \n    def extract_features(lc_df):\n        print(\"Extracting features (Phase 2: Advanced Statistics)...\")\n        \n        # 1. Th·ªëng k√™ chung (Global Stats)\n        aggs = {\n            'Flux': [\n                'min', 'max', 'mean', 'std', 'skew', \n                lambda x: np.percentile(x, 25), \n                lambda x: np.percentile(x, 50), \n                lambda x: np.percentile(x, 75), \n                lambda x: np.percentile(x, 95) - np.percentile(x, 5)\n            ],\n            'Flux_err': ['mean', 'max'],\n            'Time (MJD)': [lambda x: x.max() - x.min(), 'count']\n        }\n        \n        features = lc_df.groupby('object_id').agg(aggs)\n        features.columns = ['_'.join(col).strip() for col in features.columns.values]\n        \n        rename_dict = {\n            'Flux_<lambda_0>': 'flux_q25',\n            'Flux_<lambda_1>': 'flux_median',\n            'Flux_<lambda_2>': 'flux_q75',\n            'Flux_<lambda_3>': 'flux_range90',\n            'Time (MJD)_<lambda_0>': 'duration',\n            'Time (MJD)_count': 'n_obs'\n        }\n        features.rename(columns=rename_dict, inplace=True)\n        \n        # 2. Pivot Table (Filter Stats) - FIX ·ªû ƒê√ÇY\n        # S·ª≠a values='Flux' th√†nh values=['Flux'] ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ 3 levels c·ªôt\n        pivot_stats = pd.pivot_table(lc_df, index='object_id', columns='Filter', \n                                     values=['Flux'], # <--- QUAN TR·ªåNG: D√πng list\n                                     aggfunc=['mean', 'std', 'max', 'min'])\n        \n        # ƒê·∫∑t t√™n c·ªôt d·∫°ng: u_mean_Flux, g_std_Flux... (Filter_Agg_Value)\n        # col[2]=Filter, col[1]=Agg, col[0]=Value\n        pivot_stats.columns = [f\"{col[2]}_{col[1]}_{col[0]}\" for col in pivot_stats.columns.values]\n        \n        # 3. T√≠nh Color Index\n        bands = ['u', 'g', 'r', 'i', 'z', 'y']\n        for i in range(len(bands)-1):\n            b1, b2 = bands[i], bands[i+1]\n            # V√¨ ta ƒë√£ ƒë·∫∑t t√™n c·ªôt l√† u_mean_Flux n√™n d√πng startswith('u') l√† chu·∫©n\n            c1 = [c for c in pivot_stats.columns if c.startswith(b1) and 'mean' in c]\n            c2 = [c for c in pivot_stats.columns if c.startswith(b2) and 'mean' in c]\n            \n            if c1 and c2:\n                features[f'color_{b1}-{b2}'] = pivot_stats[c1[0]] - pivot_stats[c2[0]]\n\n        # Merge l·∫°i\n        final_features = features.merge(pivot_stats, on='object_id', how='left')\n        \n        # 4. Ratios\n        final_features['flux_std_over_mean'] = final_features['Flux_std'] / (final_features['Flux_mean'].abs() + 1e-6)\n        final_features['amplitude'] = final_features['Flux_max'] - final_features['Flux_min']\n        \n        return final_features\n\n    # Ki·ªÉm tra bi·∫øn\n    if 'full_lc' not in globals():\n        raise NameError(\"Bi·∫øn 'full_lc' ch∆∞a ƒë∆∞·ª£c t·∫°o. H√£y ch·∫°y l·∫°i Cell 4!\")\n\n    # Th·ª±c thi\n    lc_features = extract_features(full_lc)\n    \n    # Save\n    save_path = os.path.join(CFG.WORKING_DIR, feature_file_name)\n    lc_features.to_csv(save_path, index=True)\n    print(f\"üíæ Features saved to: {save_path}\")\n\n    del full_lc, dfs\n    gc.collect()\n\nprint(f\"Features shape: {lc_features.shape}\")\ndisplay(lc_features.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:03:52.149939Z","iopub.execute_input":"2025-12-19T14:03:52.150154Z","iopub.status.idle":"2025-12-19T14:04:01.617834Z","shell.execute_reply.started":"2025-12-19T14:03:52.150136Z","shell.execute_reply":"2025-12-19T14:04:01.617239Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. Prepare Train/Test Datasets","metadata":{}},{"cell_type":"code","source":"print(\"=== Preparing Final Datasets ===\")\n\n# Merge features v·ªõi Metadata (Z, EBV)\n# Train Set\ntrain_df = train_meta.merge(lc_features, on='object_id', how='left')\n# Test Set\ntest_df = test_meta.merge(lc_features, on='object_id', how='left')\n\n# Drop c√°c c·ªôt kh√¥ng d√πng train (Text, SpecType, split)\ndrop_cols = ['SpecType', 'English Translation', 'split', 'target', 'object_id']\n# Gi·ªØ l·∫°i danh s√°ch Feature\nfeature_cols = [c for c in train_df.columns if c not in drop_cols]\n\nprint(f\"Features used for training ({len(feature_cols)}):\")\nprint(feature_cols[:10], \"...\")\n\n# Check target\nprint(\"Target Distribution:\")\nprint(train_df['target'].value_counts())\n\n# Fill NaN (LightGBM x·ª≠ l√Ω ƒë∆∞·ª£c, nh∆∞ng fill 0 ho·∫∑c mean s·∫Ω an to√†n h∆°n cho 1 s·ªë metrics)\n# train_df = train_df.fillna(0)\n# test_df = test_df.fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:04:01.618547Z","iopub.execute_input":"2025-12-19T14:04:01.618822Z","iopub.status.idle":"2025-12-19T14:04:01.640582Z","shell.execute_reply.started":"2025-12-19T14:04:01.618798Z","shell.execute_reply":"2025-12-19T14:04:01.639959Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"6. Training Model (LightGBM)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nprint(\"=== Starting Training with Resume Logic ===\")\n\nX = train_df[feature_cols]\ny = train_df['target']\nX_test = test_df[feature_cols]\n\nskf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test))\nmodels = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"\\n--- Fold {fold+1}/{CFG.n_folds} ---\")\n    \n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # T√™n file model cho Fold n√†y\n    model_filename = f\"lgb_model_fold_{fold}.txt\"\n    \n    # 1. T√¨m model trong RESUME_PATH (∆Øu ti√™n load t·ª´ session tr∆∞·ªõc)\n    resume_model_path = os.path.join(CFG.RESUME_PATH, model_filename) if CFG.RESUME_PATH else None\n    \n    model = None\n    \n    if resume_model_path and os.path.exists(resume_model_path):\n        print(f\"üîÑ Resuming: Loading existing model from {resume_model_path}\")\n        model = lgb.Booster(model_file=resume_model_path)\n    else:\n        # 2. N·∫øu kh√¥ng c√≥, Train m·ªõi\n        print(f\"‚ö° Training New Model for Fold {fold+1}...\")\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n        \n        callbacks = [\n            # TƒÉng ki√™n nh·∫´n t·ª´ : Model s·∫Ω ƒë·ª£i l√¢u h∆°n xem loss c√≥ gi·∫£m ti·∫øp kh√¥ng\n            lgb.early_stopping(stopping_rounds=100), \n            # In log m·ªói 500 v√≤ng cho ƒë·ª° r·ªëi m·∫Øt\n            lgb.log_evaluation(period=500)\n        ]\n\n        # Train\n        model = lgb.train(\n            CFG.lgb_params,\n            dtrain,\n            valid_sets=[dtrain, dval],\n            # TƒÉng s·ªë v√≤ng t·ªëi ƒëa t·ª´ ƒë·ªÉ ph√π h·ª£p v·ªõi Learning Rate th·∫•p\n            num_boost_round=5000, \n            callbacks=callbacks\n        )\n        \n        # L∆∞u model ngay sau khi train xong\n        save_path = os.path.join(CFG.WORKING_DIR, model_filename)\n        model.save_model(save_path)\n        print(f\"üíæ Model saved to {save_path}\")\n\n    models.append(model)\n\n    # Predict\n    val_probs = model.predict(X_val, num_iteration=model.best_iteration)\n    oof_preds[val_idx] = val_probs\n    \n    test_preds += model.predict(X_test, num_iteration=model.best_iteration) / CFG.n_folds\n    \n    # === T√åM THRESHOLD T·ªêI ∆ØU CHO FOLD N√ÄY ===\n    best_f1 = 0\n    best_thr = 0.5\n    for thr in np.arange(0.1, 0.9, 0.05):\n        current_f1 = f1_score(y_val, (val_probs > thr).astype(int), average='macro')\n        if current_f1 > best_f1:\n            best_f1 = current_f1\n            best_thr = thr\n            \n    print(f\"Fold {fold+1} Best Threshold: {best_thr:.2f} | F1-Macro: {best_f1:.4f}\")\n\nprint(\"\\n=== Training Finished ===\")\n\n# === T√åM THRESHOLD T·ªîNG TH·ªÇ ===\nbest_global_f1 = 0\nbest_global_thr = 0.5\nfor thr in np.arange(0.1, 0.9, 0.01):\n    current_f1 = f1_score(y, (oof_preds > thr).astype(int), average='macro')\n    if current_f1 > best_global_f1:\n        best_global_f1 = current_f1\n        best_global_thr = thr\n\nprint(f\"\\nüèÜ OVERALL Best Threshold: {best_global_thr:.2f}\")\nprint(f\"üèÜ OVERALL CV F1-Macro: {best_global_f1:.4f}\")\n\n# C·∫≠p nh·∫≠t l·∫°i k·∫øt qu·∫£ submission v·ªõi ng∆∞·ª°ng t·ªëi ∆∞u n√†y\nfinal_preds_labels = (test_preds > best_global_thr).astype(int)\n\n    # # Generate Predictions (D√π load hay train m·ªõi ƒë·ªÅu ph·∫£i predict)\n    # val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    # oof_preds[val_idx] = val_pred\n    \n    # test_pred_fold = model.predict(X_test, num_iteration=model.best_iteration)\n    # test_preds += test_pred_fold / CFG.n_folds\n    \n    # # Quick Check\n    # # L∆∞u √Ω: Ng∆∞·ª°ng 0.5 c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u cho F1 n·∫øu data m·∫•t c√¢n b·∫±ng\n    # # B·∫°n c√≥ th·ªÉ th·ª≠ ch·ªânh ng∆∞·ª°ng n√†y sau khi c√≥ OOF predictions\n    # fold_f1 = f1_score(y_val, (val_pred > 0.5).astype(int), average='macro')\n    # print(f\"Fold {fold+1} F1-Macro: {fold_f1:.4f}\")\n\nprint(\"\\n=== Training Finished ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:04:01.641411Z","iopub.execute_input":"2025-12-19T14:04:01.641728Z","iopub.status.idle":"2025-12-19T14:04:05.854795Z","shell.execute_reply.started":"2025-12-19T14:04:01.641711Z","shell.execute_reply":"2025-12-19T14:04:05.853960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"7. Evaluation & Feature Importance","metadata":{}},{"cell_type":"code","source":"# ƒê√°nh gi√° t·ªïng th·ªÉ\noof_labels = (oof_preds > 0.5).astype(int)\noverall_f1 = f1_score(y, oof_labels, average='macro')\noverall_acc = accuracy_score(y, oof_labels)\n\nprint(f\"Overall CV F1-Macro: {overall_f1:.4f}\")\nprint(f\"Overall CV Accuracy: {overall_acc:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y, oof_labels))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y, oof_labels))\n\n# Plot Feature Importance\nfeature_importance = pd.DataFrame()\nfeature_importance[\"feature\"] = feature_cols\nfeature_importance[\"importance\"] = sum([m.feature_importance() for m in models])\nfeature_importance = feature_importance.sort_values(by=\"importance\", ascending=False).head(20)\n\nplt.figure(figsize=(10, 8))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance)\nplt.title(\"Top 20 Important Features\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:04:05.855701Z","iopub.execute_input":"2025-12-19T14:04:05.855988Z","iopub.status.idle":"2025-12-19T14:04:06.206074Z","shell.execute_reply.started":"2025-12-19T14:04:05.855970Z","shell.execute_reply":"2025-12-19T14:04:06.205242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"8. Submission","metadata":{}},{"cell_type":"code","source":"print(\"=== Generating Submission ===\")\n\n# Chuy·ªÉn x√°c su·∫•t th√†nh nh√£n (0/1) n·∫øu y√™u c·∫ßu file submission l√† int\n# N·∫øu y√™u c·∫ßu probability, gi·ªØ nguy√™n test_preds\nfinal_preds_labels = (test_preds > 0.5).astype(int)\n\nsubmission = pd.DataFrame({\n    'object_id': test_df['object_id'],\n    'prediction': final_preds_labels \n})\n\n# Ki·ªÉm tra format v·ªõi sample_submission n·∫øu c√≥\nif sample_sub_path:\n    sample_sub = pd.read_csv(sample_sub_path)\n    # ƒê·∫£m b·∫£o th·ª© t·ª± object_id kh·ªõp (n·∫øu c·∫ßn thi·∫øt, th∆∞·ªùng Kaggle ch·∫•m theo ID n√™n merge l√† an to√†n nh·∫•t)\n    submission = submission.set_index('object_id').reindex(sample_sub['object_id']).reset_index()\n    # Fill missing n·∫øu c√≥ (ph√≤ng tr∆∞·ªùng h·ª£p test_log thi·∫øu ID so v·ªõi sample)\n    submission['prediction'] = submission['prediction'].fillna(0).astype(int)\n\noutput_path = os.path.join(CFG.WORKING_DIR, 'submission.csv')\nsubmission.to_csv(output_path, index=False)\n\nprint(f\"Submission saved to: {output_path}\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:04:06.207796Z","iopub.execute_input":"2025-12-19T14:04:06.208047Z","iopub.status.idle":"2025-12-19T14:04:06.235918Z","shell.execute_reply.started":"2025-12-19T14:04:06.208015Z","shell.execute_reply":"2025-12-19T14:04:06.235099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# files_to_delete = [\n#     'extracted_features.csv', \n#     'lgb_model_fold_0.txt',\n#     'lgb_model_fold_1.txt',\n#     'lgb_model_fold_2.txt',\n#     'lgb_model_fold_3.txt',\n#     'lgb_model_fold_4.txt',\n#     'submission.csv'\n# ]\n\n# print(\"ƒêang d·ªçn d·∫πp file c≈©...\")\n# for f in files_to_delete:\n#     path = os.path.join('/kaggle/working', f)\n#     if os.path.exists(path):\n#         os.remove(path)\n#         print(f\"‚ùå ƒê√£ x√≥a: {f}\")\n#     else:\n#         print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y: {f}\")\n\n# print(\"Ho√†n t·∫•t. S·∫µn s√†ng ch·∫°y code m·ªõi!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:04:06.236728Z","iopub.execute_input":"2025-12-19T14:04:06.236987Z","iopub.status.idle":"2025-12-19T14:04:06.240869Z","shell.execute_reply.started":"2025-12-19T14:04:06.236970Z","shell.execute_reply":"2025-12-19T14:04:06.240136Z"}},"outputs":[],"execution_count":null}]}